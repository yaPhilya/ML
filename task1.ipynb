{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 14pt\">–î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ ‚Ññ1 </span>\n",
    "\n",
    "<span style=\"color: red; font-size: 14pt\">Deadline: 27.02.2018 23:59:59</span>\n",
    "\n",
    "<span style=\"font-size: 10pt\">–§–ò–í–¢, –ê–ü–¢, –ö—É—Ä—Å –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é, –í–µ—Å–Ω–∞ 2018</span>\n",
    "\n",
    "<span style=\"color:blue; font-size: 10pt\">Alexey Romanenko, </span>\n",
    "<span style=\"color:blue; font-size: 10pt; font-family: 'Verdana'\">alexromsput@gmail.com</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Organization Info</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–∑**:\n",
    "- –í–æ—Ä–æ–Ω—Ü–æ–≤ –ö. –í. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –ø–æ –ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–∞–º. 2012. \n",
    "- –ú–µ—Ä–∫–æ–≤ –ê. –ë. –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤. –í–≤–µ–¥–µ–Ω–∏–µ –≤ –º–µ—Ç–æ–¥—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ï–¥–∏—Ç–æ—Ä–∏–∞–ª –£–†–°–°. 2011. 256 —Å—Ç—Ä.\n",
    "- Hastie T., Tibshirani R., Friedman J. The Elements of Statistical Learning. Springer: Data Mining, Inference, and Prediction. ‚Äî 2nd ed. ‚Äî Springer-Verlag. 2009. ‚Äî 746 p.\n",
    "- C. M. Bishop. Pattern Recognition and Machine Learning. ‚Äî Springer, Series: Information Science and Statistics. 2006. ‚Äî 738 p.\n",
    "\n",
    "**–û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –¥–∑**: \n",
    "- –ü—Ä–∏—Å—ã–ª–∞–π—Ç–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ –∑–∞–¥–∞–Ω–∏–µ –Ω–∞ –ø–æ—á—Ç—É ``ml.course.mipt@gmail.com``\n",
    "- –£–∫–∞–∂–∏—Ç–µ —Ç–µ–º—É –ø–∏—Å—å–º–∞ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ ``ML2018_fall <–Ω–æ–º–µ—Ä_–≥—Ä—É–ø–ø—ã> <—Ñ–∞–º–∏–ª–∏—è>``, –∫ –ø—Ä–∏–º–µ—Ä—É -- ``ML2018_fall 596 ivanov``\n",
    "- –í—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ –¥–∑ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤ —Ñ–∞–π–ª ``<—Ñ–∞–º–∏–ª–∏—è>_<–≥—Ä—É–ø–ø–∞>_task<–Ω–æ–º–µ—Ä –∑–∞–¥–∞–Ω–∏—è>.ipnb``, –∫ –ø—Ä–∏–º–µ—Ä—É -- ``ML2018_596_task1.ipnb``\n",
    "\n",
    "**–í–æ–ø—Ä–æ—Å—ã**:\n",
    "- –ü—Ä–∏—Å—ã–ª–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ø–æ—á—Ç—É ``ml.course.mipt@gmail.com``\n",
    "- –£–∫–∞–∂–∏—Ç–µ —Ç–µ–º—É –ø–∏—Å—å–º–∞ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ ``ML2018_fall Question <–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞>``\n",
    "\n",
    "--------\n",
    "- **PS1**: –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã, –∏ –ø—Ä–æ—Å—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–º –≤–∞—à–µ –¥–∑, –µ—Å–ª–∏ –≤—ã –Ω–µ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –µ–≥–æ –ø–æ–¥–ø–∏—à–∏—Ç–µ.\n",
    "- **PS2**: –ü—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω—ã–π –¥–µ–¥–ª–∞–π–Ω —Å–Ω–∏–∂–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å –∑–∞–¥–∞–Ω–∏—è –ø–æ —Ñ–æ—Ä–º—É–ª–µ, —É–∫–∞–∑–Ω–Ω–æ–π –Ω–∞ –ø–µ—Ä–≤–æ–º —Å–µ–º–∏–Ω–∞—Ä–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è </h1> \n",
    "\n",
    "–ù–∏–∂–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤, —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –¥–ª—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞–Ω–∏—è.\n",
    "\n",
    "* –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è\n",
    "    1. –ß—Ç–æ —Ç–∞–∫–æ–µ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏? –ö–∞–∫–∏–µ –∏–∑ –Ω–∏—Ö –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ supervised learning, –∞ –∫–∞–∫–∏–µ-–∫ unsupervised learning?\n",
    "    2. –ß—Ç–æ —Ç–∞–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ? –ö–∞–∫ –∏—Ö –º–æ–∂–Ω–æ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å?\n",
    "    3. –ß—Ç–æ —Ç–∞–∫–æ–µ –æ–±—É—á–∞—é—â–∞—è –∏ —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∏, –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è? –ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ k-fold cross validation?\n",
    "    4. –†–∞–∑–ª–æ–∂–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –º–µ—Ç–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ bias –∏ Variance: –∫–∞–∫ –≤–µ–¥—É—Ç —Å–µ–±—è –æ–±–µ –≤–µ–ª–∏—á–∏–Ω—ã, –∫–æ–≥–¥–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å–µ–º–µ–π—Å—Ç–≤–∞ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤?\n",
    "\n",
    "\n",
    "* –ü—Ä–æ—Å—Ç—ã–µ –º–µ—Ç–æ–¥—ã\n",
    "    1. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç kNN –≤ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏?\n",
    "    2. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç kNN —Å –≤–µ—Å–∞–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –≤ –∑–∞–¥–∞—á–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏?\n",
    "    3. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–∏–≤–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, –≤ —á–µ–º –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –µ–≥–æ \"–Ω–∞–∏–≤–Ω–æ—Å—Ç—å\"?\n",
    "    4. –ö–∞–∫ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∏—Å—Ö–æ–¥–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å y –æ—Ç x –≤ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –∫–∞–∫ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –≤–µ—Å–∞ –≤ –Ω–µ–π?\n",
    "    5. –í —á–µ–º —Å—É—Ç—å –ø—Ä–æ–∫–ª—è—Ç–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏?\n",
    "\n",
    "\n",
    "* –ò–∑–º–µ—Ä–µ–Ω–∏–µ –æ—à–∏–±–∫–∏/—Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "    1. –ö–∞–∫ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –∏ –≤ –∫–∞–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏/—Ä–µ–≥—Ä–µ—Å—Å–∏–∏) –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∏:\n",
    "    accuracy, precision, recall, F1-measure, ROC-AUC, PR-AUC, MSE, MAE, RMSE?\n",
    "    2. –†–µ—à–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Å –¥–≤—É–º—è –∫–ª–∞—Å—Å–∞–º–∏ - 0 –∏ 1), –≤ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏-\n",
    "    –º–µ—Ä—ã –∏–∑ –∫–ª–∞—Å—Å–∞ 0 —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç 95% –≤—ã–±–æ—Ä–∫–∏. –ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≤ –ø—Ä–µ–¥—ã-\n",
    "    –¥—É—â–µ–º –≤–æ–ø—Ä–æ—Å–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?\n",
    "    3. –ö –æ—Ü–µ–Ω–∫–µ –∫–∞–∫–æ–π –≤–µ–ª–∏—á–∏–Ω—ã –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è y –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ x –ø—Ä–∏–≤–æ–¥—è—Ç MSE –∏ MAE?\n",
    "    4. –ö–∞–∫–∞—è –∏–∑ –º–µ—Ç—Ä–∏–∫ –¥–æ–ª–∂–Ω—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ –∑–∞–¥–∞—á–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç –±—ã–ª —Ä–∞–≤–µ–Ω –º–µ–¥–∏–∞–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ–π —Å–ª—É—á–∞–π–Ω–æ –≤–µ–ª–∏—á–∏–Ω—ã?\n",
    "\n",
    "\n",
    "* Python, numpy, scipy, matplotlib, sklearn, pandas\n",
    "    1. –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö list, tuple, dict, set, str, unicode, hashable –∏ unhashable —Ç–∏–ø—ã. \n",
    "    2. –ó–∞—á–µ–º –Ω—É–∂–Ω—ã numpy –∏ scipy? –ö–∞–∫–æ–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –≤ numpy –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å\n",
    "    –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–º–∏ –º–∞—Å—Å–∏–≤–∞–º–∏? –û—Ç–ª–∏—á–∏—è –≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–≤—É–º–µ—Ä–Ω–æ–≥–æ ndarray –∏ —Å–ø–∏—Å–∫–∞ —Å–ø–∏—Å–∫–æ–≤.\n",
    "    3. –ö–∞–∫ –≤ scipy —Ä–µ—à–∏—Ç—å —á–∏—Å–ª–µ–Ω–Ω–æ –Ω–µ—Å–ª–æ–∂–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—É—é –∑–∞–¥–∞—á—É? –ö–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–ø-\n",
    "    —Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –Ω–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã?\n",
    "    4. –ö–∞–∫ –ø–æ —Å–ø–∏—Å–∫—É –∑–Ω–∞—á–µ–Ω–∏–π x –∏ —Å–ø–∏—Å–∫—É –∑–Ω–∞—á–µ–Ω–∏–π y –≤ —ç—Ç–∏—Ö —Ç–æ—á–∫–∞—Ö –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ y(x) –≤\n",
    "    matplotlib?\n",
    "    5. –ö–∞–∫ –≤ sklearn –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –∏ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π?\n",
    "    6. –ö–∞–∫–∏–µ –µ—Å—Ç—å —Å—Ä–µ–¥—Å—Ç–≤–∞ –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –≤ sklearn? –ö–∞–∫ –ø–æ—Å—á—Ç–∏—Ç–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ\n",
    "    –≤ k-fold cross validation?\n",
    "    7. –ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ cross_val_score –∏–∑ sklearn?\n",
    "    8. –ö–∞–∫ —Å—á–∏—Ç–∞—Ç—å –≤—ã–±–æ—Ä–∫—É –∏–∑ csv –≤ pandas DataFrame? –ê –∫–∞–∫ –∑–∞–ø–∏—Å–∞—Ç—å DataFrame –≤ —Ñ–∞–π–ª? –ö–∞–∫\n",
    "    —É–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏/–∑–∞–ø–∏—Å–∏ –∫–æ–¥–∏—Ä–æ–≤–∫—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏, –Ω–∞–ª–∏—á–∏–µ/–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ\n",
    "    –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —É –∫–æ–ª–æ–Ω–æ–∫?\n",
    "    9. –ö–∞–∫ –ø–æ —Å–ø–∏—Å–∫—É –∑–Ω–∞—á–µ–Ω–∏–π x –∏ —Å–ø–∏—Å–∫—É –∑–Ω–∞—á–µ–Ω–∏–π y –≤ —ç—Ç–∏—Ö —Ç–æ—á–∫–∞—Ö –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ y(x) –≤\n",
    "    matplotlib?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã (2 –±–∞–ª–ª–∞) </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** –ó–∞–¥–∞—á–∞ 1**\n",
    "–ü–æ–∫–∞–∂–∏—Ç–µ, —á—Ç–æ ROC-AUC  –≤ —Å–ª—É—á–∞–µ, –∫–æ–≥–¥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–∞—ë—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã $a(x) = 1$ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é $p$ –∏ $a(x) = 0$ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é $1 ‚àí p$, –±—É–¥–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º —Ä–∞–≤–µ–Ω 0.5, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç p –∏ –¥–æ–ª–∏ –∫–ª–∞—Å—Å–∞ 1 –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ.\n",
    "\n",
    "ROC-AUC –ø–ª–æ—â–∞–¥—å –ø–æ–¥ –∫—Ä–∏–≤–æ–π ROC, –≥–¥–µ –∫—Ä–∏–≤–∞—è ROC –∫—Ä–∏–≤–∞—è –æ—Ç–Ω–æ—à–µ–Ω–∏—è True Positive Rate –∫ False Positive Rate.\n",
    "\n",
    "$TPR = \\cfrac{\\sum_1^n{I\\{\\hat{y} = 1; y=1\\}}}{\\#\\{y=1\\}}$\n",
    "$FPR = \\cfrac{\\sum_1^n{I\\{\\hat{y} = 1; y=0\\}}}{\\#\\{y=0\\}}$\n",
    "\n",
    "–ü–æ—Å—á–∏—Ç–∞–µ–º –º–∞—Ç–æ–∂–∏–¥–∞–Ω–∏–µ TPR –∏ FPR. $E(TPR) = \\cfrac{p\\cdot\\#\\{y=1\\}}{\\#\\{y=1\\}} = p$\n",
    "$~~E(TPR) = \\cfrac{p\\cdot\\#\\{y=0\\}}{\\#\\{y=0\\}} = p$\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –≤ —Å—Ä–µ–¥–Ω–µ–º $\\cfrac{TPR}{FPR} = 1$, –∑–Ω–∞—á–∏—Ç ROC —ç—Ç–æ –ø—Ä—è–º–∞—è –ø–æ–¥ —É–≥–ª–æ–º 45, –∞ AUC-ROC = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** –ó–∞–¥–∞—á–∞ 2**\n",
    "–ü–æ–∫–∞–∂–∏—Ç–µ, —á—Ç–æ —Å —Ä–æ—Å—Ç–æ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ç–æ—á–µ–∫ –≤ –∫—É–±–µ $[0; 1]^ùëë$ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ø–∞—Å—Ç—å –≤ –∫—É–± $[0; 0, 99]^ùëë$ —Å—Ç—Ä–µ–º–∏—Ç—Å—è –∫ –Ω—É–ª—é. –≠—Ç–æ –æ–¥–Ω–∞ –∏–∑ –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π –ø—Ä–æ–∫–ª—è—Ç–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π (dimension curse). –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–∏–¥—É–º–∞—Ç—å –∏–ª–∏ –Ω–∞–π—Ç–∏ –µ—â–µ –∫–∞–∫—É—é-–Ω–∏–±—É–¥—å –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—é –∫ —ç—Ç–æ–º—É —è–≤–ª–µ–Ω–∏—é –∏ –∫—Ä–∞—Ç–∫–æ –∏–∑–ª–æ–∂–∏—Ç—å.\n",
    "\n",
    "–ß—Ç–æ–±—ã –ø–æ–ø–∞—Å—Ç—å –≤ –∫—É–± $[0; 0, 99]^ùëë$ –Ω—É–∂–Ω–æ –ø–æ–ø–∞—Å—Ç—å –≤ –æ—Ç—Ä–µ–∑–æ–∫ $[0; 0, 99]$ –ø–æ –∫–∞–∂–¥–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏. –¢–∞–∫ –∫–∞–∫ —Ç–æ—á–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –≤ –∫—É–±–µ $[0; 1]^ùëë$, —Ç–æ –æ–Ω–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –ø–æ –∫–∞–∂–¥–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏. –ê –∑–Ω–∞—á–∏—Ç $P(x \\in [0; 0, 99]^ùëë) = \\prod_1^d{P(x_i \\in [0; 0,99])} = \\left(\\cfrac{99}{100}\\right)^d \\rightarrow 0, d \\rightarrow \\infty$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å (3 –±–∞–ª–ª–∞) </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É—è –¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ—Ö–æ–¥–æ–≤ –∏–Ω–¥–∏–≤–∏–¥—É—É–º–∞ http://archive.ics.uci.edu/ml/machine-learning-databases/adult, –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤–µ–ª–∏—á–∏–Ω bias, variance –∏ noise –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö –ø–∞—Ä \"–º–æ–¥–µ–ª—å|–ø–∞—Ä–∞–º–µ—Ç—Ä\"\n",
    "\n",
    "* kNN, $n\\_neigbours$ - —á–∏—Å–ª–æ —Å–æ—Å–µ–¥–µ–π –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏;\n",
    "* $p$ - —Å—Ç–µ–ø–µ–Ω—å –º–µ—Ç—Ä–∏–∫–∏ –ú–∏–Ω–∫–æ–≤—Å–∫–æ–≥–æ;\n",
    "* $\\ell\\_train$ - –¥–ª–∏–Ω–∞ –æ–±—É—á–∞—é—â–∏–π –≤—ã–±–æ—Ä–∫–∏;\n",
    "\n",
    "\n",
    "–ü–æ –∫–∞–∂–¥–æ–º—É –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–æ–º—É –≥—Ä–∞—Ñ–∏–∫—É (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫) –æ–±—ä—è—Å–Ω–∏—Ç–µ –ø–æ–ª—É—á–µ–Ω–Ω—É—é –∫–∞—Ä—Ç–∏–Ω—É. –°–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –æ–Ω–∞ —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ–∂–∏–¥–∞–Ω–∏—è–º–∏ (—Å–º. —Å–µ–º–∏–Ω–∞—Ä 2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "n_repeat = 100       # Number of iterations for computing expectations\n",
    "test_size = 0.5       # (Relative) Size of the test set\n",
    "\n",
    "# –ò—Å—Å–ª–µ–¥—É–µ–º–∞—è —Å–µ—Ç–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "# –ø–æ—è—Å–Ω–µ–Ω–∏–µ: –¥–ª–∏–Ω–∞ train –≤—ã–±–æ—Ä–∫–∏ –¥–æ–ª–∂–Ω–∞ –≤–∞—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –æ—Ç 50% –¥–æ 200% –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥–ª–∏–Ω—ã —Ç–µ—Å—Ç–æ–≤–æ –≤—ã–±–æ—Ä–∫–∏  \n",
    "parameters = {\"k\" : np.arange(1, 20), \"p\" : np.arange(1, 10, 0.5), \"l_train\" : np.arange(0.5, 2, 0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Data\n",
    "data = pd.read_csv(\"adult .data\", names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                                         'marital-status', 'occupation', 'relationship', 'race', 'sex', \n",
    "                                         'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', \n",
    "                                         'resp'], index_col=None, skipinitialspace=True, na_values=['?'])\n",
    "#train , test = —Å–∫–∞—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ä–∞–∑–±–µ–π—Ç–µ –∏—Ö –Ω–∞ —Ç–µ—Å—Ç –∏ —Ç—Ä–µ–π–Ω (–≤ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ 50:50)\n",
    "data.drop(axis=1, columns=['education', 'native-country'], inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data['sex'] = data.apply(lambda row: int(row['sex'] == 'Male'), axis=1)\n",
    "data['resp'] = data.apply(lambda row: int(row['resp'] == '>50K'), axis=1)\n",
    "\n",
    "for r in ['White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other', 'Black']:\n",
    "    data[r] = data.apply(lambda row: int(row['race'] == r), axis=1)\n",
    "data.drop(axis=1, columns=['workclass', 'marital-status', 'occupation', 'relationship', 'race'], inplace=True)\n",
    "cols = data.columns.tolist()\n",
    "cols = cols[:7] + cols[8:] + cols[7:8]\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "######\n",
    "n_repeat = 60\n",
    "######\n",
    "\n",
    "# –î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å bias –∏ variance –Ω—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ —Å \n",
    "#\n",
    "#–≤–æ–∑–≤—Ä—â–µ–Ω–∏–µ–º –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –º–∞—Å—Å–∏–≤–∞ –¥–∞–Ω–Ω—ã—Ö \n",
    "# (–±—É—Ç—Å—Ç—Ä–µ–ø–∏–Ω–≥)\n",
    "for i in range(n_repeat):\n",
    "    sample_train = train.sample(len(train), replace=True) \n",
    "    X, y = sample_train.iloc[:, :-1], sample_train.iloc[:, -1:]\n",
    "    X_train.append(X)\n",
    "    y_train.append(y)\n",
    "\n",
    "# –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ä–∞ –æ—Ü–µ–Ω–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ error, bias –∏ variance —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ö–µ–º–µ\n",
    "p_name ='p'\n",
    "bias_variance_df = pd.DataFrame.from_dict({p_name:parameters[p_name],'bias':list([np.NaN]*len(parameters[p_name]))\n",
    "                                           , 'variance':list([np.NaN]*len(parameters[p_name]))\n",
    "                                           , 'error':list([np.NaN]*len(parameters[p_name]))})\n",
    "X_test, y_test = test.iloc[:, :-1].values, test.iloc[:, -1:].values\n",
    "n_test = len(test)\n",
    "\n",
    "for parameter in tqdm_notebook(parameters[p_name]):\n",
    "    # Compute predictions\n",
    "    \n",
    "    y_predict = np.zeros((n_test, n_repeat))\n",
    "    \n",
    "    \n",
    "    for i in tqdm_notebook(range(n_repeat), leave=False):\n",
    "        estimator = KNeighborsClassifier(p=parameter, n_neighbors=10, n_jobs=-1).fit(X_train[i].values,\n",
    "                                                                                     y_train[i].values.ravel())\n",
    "        y_predict[:, i] = estimator.predict(X_test)\n",
    "\n",
    "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "    y_error = np.zeros(n_test)\n",
    "\n",
    "\n",
    "    \n",
    "    #for i in range(n_repeat):\n",
    "        #print(y_error.shape, y_test.shape, y_predict[:, i].shape)\n",
    "    #    y_error += (y_test.ravel() - y_predict[:, i]) ** 2\n",
    "    y_error = np.sum((y_test.repeat(n_repeat, axis=1) - y_predict)**2, axis=1)\n",
    "    y_error /= n_repeat\n",
    "\n",
    "    # –í –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–µ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ bias –ø–æ–ª–æ–∂–∏–º —Ä–∞–≤–Ω—ã–º –∫–≤–∞–¥—Ä–∞—Ç—É —Ä–∞–∑–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è \n",
    "    # –∏ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≤—Å–µ–º –æ–±—É—á–∞—é—â–∏–º –≤—ã–±–æ—Ä–∫–∞–º.\n",
    "    y_bias = (y_test.ravel() - np.mean(y_predict, axis=1).ravel()) ** 2\n",
    "    \n",
    "    # Variance –ø–æ–ª–æ–∂–∏–º —Ä–∞–≤–Ω—ã–º —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–º—É —Ä–∞–∑–±—Ä–æ—Å—É –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –≤—Å–µ–º –æ–±—É—á–∞—é—â–∏–º –≤—ã–±–æ—Ä–∫–∞–º. \n",
    "    # –ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ bias –∏ variance –≤–æ–∑—å–º—ë–º —Ä–∞–≤–Ω—ã–º —Å—Ä–µ–¥–Ω–µ–º—É –ø–æ –≤—Å–µ–º —Ç–æ—á–∫–∞–º —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    y_var = np.var(y_predict, axis=1)\n",
    "    \n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'bias'] = y_bias.mean()\n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'variance'] = y_var.mean()\n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'error'] = y_error.mean()\n",
    "    \n",
    "# –Ω–∞—Ä–∏—Å—É–π—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å bias, variance –∏ error –æ—Ç –∑–Ω–∞—á–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "#plt.figure(figsize=(10, 8))\n",
    "bias_variance_df.plot.line('p', ['bias', 'error', 'variance'], figsize=(10, 10))\n",
    "#bias_variance_df.plot.line('p', 'error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias_variance_df.plot.line('p', 'error')\n",
    "plt.show()\n",
    "bias_variance_df.plot.line('p', 'bias')\n",
    "plt.show()\n",
    "bias_variance_df.plot.line('p', 'variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ä–∞ –æ—Ü–µ–Ω–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ error, bias –∏ variance —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ö–µ–º–µ\n",
    "p_name ='k'\n",
    "bias_variance_df = pd.DataFrame.from_dict({p_name:parameters[p_name],'bias':list([np.NaN]*len(parameters[p_name]))\n",
    "                                           , 'variance':list([np.NaN]*len(parameters[p_name]))\n",
    "                                           , 'error':list([np.NaN]*len(parameters[p_name]))})\n",
    "X_test, y_test = test.iloc[:, :-1].values, test.iloc[:, -1:].values\n",
    "n_test = len(test)\n",
    "\n",
    "for parameter in parameters[p_name]:\n",
    "    # Compute predictions\n",
    "    \n",
    "    y_predict = np.zeros((n_test, n_repeat))\n",
    "    \n",
    "    \n",
    "    for i in range(n_repeat):\n",
    "        estimator = KNeighborsClassifier(n_neighbors=parameter, n_jobs=-1).fit(X_train[i].values, \n",
    "                                                                               y_train[i].values.ravel())\n",
    "        y_predict[:, i] = estimator.predict(X_test)\n",
    "\n",
    "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "    y_error = np.zeros(n_test)\n",
    "\n",
    "\n",
    "    \n",
    "    #for i in range(n_repeat):\n",
    "        #print(y_error.shape, y_test.shape, y_predict[:, i].shape)\n",
    "    #    y_error += (y_test.ravel() - y_predict[:, i]) ** 2\n",
    "    y_error = np.sum((y_test.repeat(n_repeat, axis=1) - y_predict)**2, axis=1)\n",
    "    y_error /= n_repeat\n",
    "\n",
    "    # –í –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–µ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ bias –ø–æ–ª–æ–∂–∏–º —Ä–∞–≤–Ω—ã–º –∫–≤–∞–¥—Ä–∞—Ç—É —Ä–∞–∑–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è \n",
    "    # –∏ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≤—Å–µ–º –æ–±—É—á–∞—é—â–∏–º –≤—ã–±–æ—Ä–∫–∞–º.\n",
    "    y_bias = (y_test.ravel() - np.mean(y_predict, axis=1).ravel()) ** 2\n",
    "    \n",
    "    # Variance –ø–æ–ª–æ–∂–∏–º —Ä–∞–≤–Ω—ã–º —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–º—É —Ä–∞–∑–±—Ä–æ—Å—É –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –≤—Å–µ–º –æ–±—É—á–∞—é—â–∏–º –≤—ã–±–æ—Ä–∫–∞–º. \n",
    "    # –ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ bias –∏ variance –≤–æ–∑—å–º—ë–º —Ä–∞–≤–Ω—ã–º —Å—Ä–µ–¥–Ω–µ–º—É –ø–æ –≤—Å–µ–º —Ç–æ—á–∫–∞–º —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    y_var = np.var(y_predict, axis=1)\n",
    "    \n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'bias'] = y_bias.mean()\n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'variance'] = y_var.mean()\n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'error'] = y_error.mean()\n",
    "    \n",
    "# –Ω–∞—Ä–∏—Å—É–π—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å bias, variance –∏ error –æ—Ç –∑–Ω–∞—á–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "#plt.figure(figsize=(10, 8))\n",
    "bias_variance_df.plot.line('k', ['bias', 'error', 'variance'], figsize=(10, 10))\n",
    "#bias_variance_df.plot.line('p', 'error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias_variance_df.plot.line('k', 'error')\n",
    "plt.show()\n",
    "bias_variance_df.plot.line('k', 'bias')\n",
    "plt.show()\n",
    "bias_variance_df.plot.line('k', 'variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_name ='l_train'\n",
    "bias_variance_df = pd.DataFrame.from_dict({p_name:parameters[p_name],'bias':list([np.NaN]*len(parameters[p_name]))\n",
    "                                           , 'variance':list([np.NaN]*len(parameters[p_name]))\n",
    "                                           , 'error':list([np.NaN]*len(parameters[p_name]))})\n",
    "for parameter in parameters[p_name]:\n",
    "    # Compute predictions\n",
    "    train, test = train_test_split(data, test_size=1 / (1 + parameter))\n",
    "    #################\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    ######\n",
    "    n_repeat = 60\n",
    "    ######\n",
    "\n",
    "    # –î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å bias –∏ variance –Ω—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ —Å \n",
    "    #\n",
    "    #–≤–æ–∑–≤—Ä—â–µ–Ω–∏–µ–º –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –º–∞—Å—Å–∏–≤–∞ –¥–∞–Ω–Ω—ã—Ö \n",
    "    # (–±—É—Ç—Å—Ç—Ä–µ–ø–∏–Ω–≥)\n",
    "    for i in range(n_repeat):\n",
    "        sample_train = train.sample(len(train), replace=True) \n",
    "        X, y = sample_train.iloc[:, :-1], sample_train.iloc[:, -1:]\n",
    "        X_train.append(X)\n",
    "        y_train.append(y)\n",
    "\n",
    "    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ä–∞ –æ—Ü–µ–Ω–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ error, bias –∏ variance —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ö–µ–º–µ\n",
    "    X_test, y_test = test.iloc[:, :-1].values, test.iloc[:, -1:].values\n",
    "    n_test = len(test)\n",
    "    #######################\n",
    "    y_predict = np.zeros((n_test, n_repeat))\n",
    "    \n",
    "    \n",
    "    for i in range(n_repeat):\n",
    "        estimator = KNeighborsClassifier(n_neighbors=10, n_jobs=-1).fit(X_train[i].values,\n",
    "                                                                                     y_train[i].values.ravel())\n",
    "        y_predict[:, i] = estimator.predict(X_test)\n",
    "\n",
    "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "    y_error = np.zeros(n_test)\n",
    "\n",
    "\n",
    "    \n",
    "    #for i in range(n_repeat):\n",
    "        #print(y_error.shape, y_test.shape, y_predict[:, i].shape)\n",
    "    #    y_error += (y_test.ravel() - y_predict[:, i]) ** 2\n",
    "    y_error = np.sum((y_test.repeat(n_repeat, axis=1) - y_predict)**2, axis=1)\n",
    "    y_error /= n_repeat\n",
    "\n",
    "    # –í –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–µ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ bias –ø–æ–ª–æ–∂–∏–º —Ä–∞–≤–Ω—ã–º –∫–≤–∞–¥—Ä–∞—Ç—É —Ä–∞–∑–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è \n",
    "    # –∏ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –≤—Å–µ–º –æ–±—É—á–∞—é—â–∏–º –≤—ã–±–æ—Ä–∫–∞–º.\n",
    "    y_bias = (y_test.ravel() - np.mean(y_predict, axis=1).ravel()) ** 2\n",
    "    \n",
    "    # Variance –ø–æ–ª–æ–∂–∏–º —Ä–∞–≤–Ω—ã–º —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–º—É —Ä–∞–∑–±—Ä–æ—Å—É –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –≤—Å–µ–º –æ–±—É—á–∞—é—â–∏–º –≤—ã–±–æ—Ä–∫–∞–º. \n",
    "    # –ò—Ç–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ bias –∏ variance –≤–æ–∑—å–º—ë–º —Ä–∞–≤–Ω—ã–º —Å—Ä–µ–¥–Ω–µ–º—É –ø–æ –≤—Å–µ–º —Ç–æ—á–∫–∞–º —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    y_var = np.var(y_predict, axis=1)\n",
    "    \n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'bias'] = y_bias.mean()\n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'variance'] = y_var.mean()\n",
    "    bias_variance_df.loc[bias_variance_df[p_name]==parameter, 'error'] = y_error.mean()\n",
    "    \n",
    "# –Ω–∞—Ä–∏—Å—É–π—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å bias, variance –∏ error –æ—Ç –∑–Ω–∞—á–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "#plt.figure(figsize=(10, 8))\n",
    "bias_variance_df.plot.line('l_train', ['bias', 'error', 'variance'], figsize=(10, 10))\n",
    "#bias_variance_df.plot.line('p', 'error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias_variance_df.plot.line('l_train', 'error')\n",
    "plt.show()\n",
    "bias_variance_df.plot.line('l_train', 'bias')\n",
    "plt.show()\n",
    "bias_variance_df.plot.line('l_train', 'variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">–†–µ–∞–ª–∏–∑—É–π—Ç–µ kNN (2 –±–∞–ª–ª–∞)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "class kNNClassifier():\n",
    "    def __init__(self, n_estimators, metric):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_neighbours: int\n",
    "            –ß–∏—Å–ª–æ —Å–æ—Å–µ–¥–µ–π\n",
    "\n",
    "        metric: *alias\n",
    "            –º–µ—Ç—Ä–∏–∫–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π\n",
    "\n",
    "          \"\"\"\n",
    "        from scipy.spatial.distance import cdist\n",
    "        assert n_estimators > 0\n",
    "        self.n_neighbours = n_estimators\n",
    "        self.metric = metric\n",
    "        #self.func = cdist\n",
    "    def func(self, XA, XB):\n",
    "        from scipy.spatial.distance import cdist\n",
    "        return cdist(XA, XB, self.metric[0], **self.metric[1])\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array\n",
    "        y: 1d np.array\n",
    "        \"\"\"\n",
    "\n",
    "        # –¢—É—Ç —Ö—Ä–∞–Ω–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "        self.X_learn = X\n",
    "\n",
    "        # –¢—É—Ç —Ö—Ä–∞–Ω–∏—Ç–µ –æ—Ç–≤–µ—Ç—ã –ø–æ –∫–∞–∂–¥–æ–º—É –æ–±—ä–µ–∫—Ç—É –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "        self.y_learn = y\n",
    "        \n",
    "        assert self.X_learn.shape[0] == self.y_learn.shape[0]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array –º–∞—Ç—Ä–∏—Ü–∞ –æ–±—ä–µ–∫—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –Ω—É–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å –æ—Ç–≤–µ—Ç\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: 1d np.array, –≤–µ–∫—Ç–æ—Ä –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞\n",
    "        \"\"\"\n",
    "        assert X.shape[1] == self.X_learn.shape[1]\n",
    "        \n",
    "        dist = [] # –•—Ä–∞–Ω–∏—Ç–µ —Ç—É—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ \n",
    "        \n",
    "        dist = self.func(X, self.X_learn)\n",
    "        dist = dist.argsort()[:, :self.n_neighbours]\n",
    "        \n",
    "        #for i in range(self.X_learn):\n",
    "            # =======================================\n",
    "            # —Ä–∞—Å—Å—á–∏—Ç–∞–π—Ç–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "            # ======================================\n",
    "            \n",
    "\n",
    "        # =======================================\n",
    "        # –ø—Ä–µ–¥—Å–∫–∞–∂–∏—Ç–µ –∫–ª–∞—Å—Å –∫–∞–∂–¥–æ–≥–æ –∏–∑ –æ–±—ä–µ–∫—Ç–æ–≤\n",
    "        # =======================================\n",
    "        \n",
    "        y_pred = np.around(np.mean(self.y_learn[dist], axis=1))\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([[5, 4, 3, 2, 1], [10, 9, 8, 7, 6], [15, 14, 13, 12, 11]])\n",
    "b = np.array([20, 19, 18, 17, 16])\n",
    "b[a.argsort()[:, :3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "cdist([[1,1]], [[2,2]], 'minkowski', **{'p':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤–∞—à –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞ –¥–∞–Ω–Ω—ã—Ö http://archive.ics.uci.edu/ml/machine-learning-databases/adult\n",
    "from sklearn.metrics import accuracy_score\n",
    "# =======================================\n",
    "# –û–±—É—á–∏—Ç–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–∏ k=3, 5, –∏ 10\n",
    "# =======================================\n",
    "for k in [3, 5, 10]:\n",
    "    clf = kNNClassifier(k, ('minkowski', {'p':2})).fit(X_train[0].values, y_train[0].values.ravel())\n",
    "    print('Train accuracy with {} neigbours: '.format(k), accuracy_score(clf.predict(X_train[0].values[:1000, :]), \n",
    "                                                                          y_train[0].values.ravel()[:1000]))\n",
    "    print('Test accuracy with {} neigbours: '.format(k), accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">–û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (3 –±–∞–ª–ª–∞) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ADD_DELL –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –µ–≥–æ –¥–ª—è kNN –Ω–∞ –¥–∞–Ω–Ω—ã—Ö sklearn.datasets.digits(). \n",
    "–î–ª—è —ç—Ç–æ–≥–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –æ—à–∏–±–∫–∏\n",
    "def update_Q_min_index(estimator, Q_min, Q_min_set, Q_min_index, feature_set, \n",
    "                       X_train, Y_train, X_test, Y_test, update_equal=False):\n",
    "    feature_set_size = len(feature_set)\n",
    "    estimator.fit(X_train[:, feature_set], Y_train)\n",
    "    error = 1 - accuracy_score(Y_test, estimator.predict(X_test[:, feature_set]))\n",
    "    if error < Q_min[feature_set_size]:\n",
    "        Q_min_set[feature_set_size] = feature_set\n",
    "        Q_min[feature_set_size] = error\n",
    "        if ((Q_min[feature_set_size] < Q_min[Q_min_index])\n",
    "                or (update_equal and Q_min[feature_set_size] == Q_min[Q_min_index])):\n",
    "            Q_min_index = feature_set_size\n",
    "    return Q_min_index\n",
    "\n",
    "\n",
    "# –ù—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ\n",
    "def add_one(estimator, feature_set, X_train, Y_train, X_test, Y_test, look_forward=10, start_feature_set=[]):\n",
    "    Q_min = {0: float('+inf')}\n",
    "    Q_min_set = {0: []}\n",
    "    Q_min_index = 0\n",
    "    \n",
    "    # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤     \n",
    "    if start_feature_set:\n",
    "        estimator.fit(X_train[:, feature_set], Y_train)\n",
    "        error = 1 - accuracy_score(Y_test, estimator.predict(X_test[:, feature_set]))\n",
    "        Q_min = {len(start_feature_set) : error}\n",
    "        Q_min_set = {len(start_feature_set) : start_feature_set}\n",
    "        Q_min_index = len(start_feature_set)\n",
    "    \n",
    "    # –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤     \n",
    "    for feature_set_size in range(len(start_feature_set) + 1, len(feature_set) + 1):\n",
    "        Q_min[feature_set_size] = float('+inf')\n",
    "        unused_features = set(feature_set).difference(set(Q_min_set[feature_set_size - 1]))\n",
    "        unused_features = list(unused_features)\n",
    "        #shuffle(unused_features)\n",
    "        for feature in unused_features:\n",
    "            new_feature_set = –æ–±–Ω–æ–≤–∏—Ç–µ —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "            Q_min_index = –æ–±–Ω–æ–≤–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        \n",
    "        print ('Q_min: %.4lf, set_size: %d, added: %s' % (Q_min[feature_set_size], \n",
    "                                                          feature_set_size, Q_min_set[feature_set_size][-1]))        \n",
    "    \n",
    "    # –∫—Ä–∏—Ç–µ—Ä–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∞\n",
    "        if Q_min_index + look_forward <= feature_set_size:\n",
    "            break\n",
    "    return Q_min_set[Q_min_index]\n",
    "\n",
    "def del_one(estimator, feature_set, X_train, Y_train, X_test, Y_test, look_forward=10):\n",
    "    \n",
    "#     –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    estimator.fit(X_train[:, feature_set], Y_train)\n",
    "    error = 1 - accuracy_score(Y_test, estimator.predict(X_test[:, feature_set]))\n",
    "    Q_min = {len(feature_set): error}\n",
    "    Q_min_set = {len(feature_set): deepcopy(feature_set)}\n",
    "    Q_min_index = len(feature_set)\n",
    "    \n",
    "#     –∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    for feature_set_size in list(range(len(feature_set) - 1, 0, -1)):\n",
    "        Q_min[feature_set_size] = float('+inf')\n",
    "        features = copy(Q_min_set[feature_set_size + 1])\n",
    "        # shuffle(features)\n",
    "        for feature in features:\n",
    "            new_feature_set = –æ–±–Ω–æ–≤–∏—Ç–µ —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "            Q_min_index = –æ–±–Ω–æ–≤–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "            \n",
    "        print ('Q_min: %.4lf, set_size: %d, deleted: %s' % (Q_min[feature_set_size], feature_set_size,\n",
    "            set(Q_min_set[feature_set_size + 1]).difference(set(Q_min_set[feature_set_size])),\n",
    "        ))\n",
    "        \n",
    "#         –∫—Ä–∏—Ç–µ—Ä–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∞\n",
    "        if feature_set_size + look_forward <= Q_min_index:\n",
    "            break\n",
    "    return Q_min_set[Q_min_index]\n",
    "\n",
    "# –ù–∞–∫–æ–Ω–µ—Ü —Ä–µ–∞–ª–∏–∑—É–π—Ç–µ ADD-DELL\n",
    "def add_del(estimator, feature_set, X_train, Y_train, X_test, Y_test, look_forward=10):\n",
    "    Q_min = None\n",
    "    Q_min_current = float('+inf')\n",
    "    start_feature_set = []\n",
    "    \n",
    "#     –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π, –ø—Ä–∏–≤–µ–¥—à–∏—Ö –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é –æ—à–∏–±–∫–∏\n",
    "    bad_iteration_count = 2\n",
    "    \n",
    "    while (Q_min is None) or (Q_min_current < Q_min or bad_iteration_count):\n",
    "        Q_min = Q_min_current\n",
    "        start_feature_set = add_one(estimator, feature_set, X_train, Y_train, X_test, Y_test, \n",
    "                                    look_forward, start_feature_set=start_feature_set)\n",
    "        start_feature_set = del_one(estimator, start_feature_set, X_train, Y_train, X_test, Y_test, look_forward)\n",
    "        estimator.fit(X_train[:, start_feature_set], Y_train)\n",
    "        Q_min_current = 1 - accuracy_score(Y_test, estimator.predict(X_test[:, start_feature_set]))\n",
    "        \n",
    "        if Q_min_current > Q_min:\n",
    "            bad_iteration_count -= 1\n",
    "        print ('Q_min: %.4lf, set_size: %d' % (Q_min_current, len(start_feature_set)))\n",
    "    return start_feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = —Å–∫–∞–π—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ä–∞–∑–±–µ–π—Ç–µ –∏—Ö –Ω–∞ —Ç–µ—Å—Ç/—Ç—Ä–µ–π–Ω\n",
    "\n",
    "# –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞—à—É —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é\n",
    "estimator = KNeighborsClassifier()\n",
    "%time feature_set = add_del(estimator, list(range(X_train.shape[1])), X_train, Y_train, X_test, Y_test, look_forward=4)\n",
    "%time accuracy_score(Y_test, estimator.fit(X_train[:, feature_set], Y_train).predict(X_test[:, feature_set]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "ff2f53b331fe4cd787f8f3f6a635e419": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
